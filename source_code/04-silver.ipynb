{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2d9190-b202-468f-bcec-a6a34c61f3b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f643c1-cfec-42d8-bd95-12693c8eaa93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, broadcast, when, split, to_json, to_timestamp, get, explode_outer\n",
    "\n",
    "silver_checkpoint = f\"{checkpoint_base_path}/silver\"\n",
    "\n",
    "# Ensure all required silver tables exist in the catalog\n",
    "def ensure_silver_tables(): \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.line_status(\n",
    "        line_id STRING,\n",
    "        service_type STRING,\n",
    "        severity_code BIGINT,\n",
    "        severity_description STRING,\n",
    "        disruption_category STRING,\n",
    "        disruption_description STRING,\n",
    "        disruption_from_date TIMESTAMP,\n",
    "        disruption_to_date TIMESTAMP,\n",
    "        is_service_disrupted BOOLEAN,\n",
    "        event_timestamp TIMESTAMP\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.bus_arrivals(\n",
    "        arrival_id STRING,\n",
    "        operation_type BIGINT,\n",
    "        vehicle_id STRING,\n",
    "        naptan_id STRING,\n",
    "        station_name STRING,\n",
    "        line_id STRING,\n",
    "        platform_name STRING,\n",
    "        direction STRING,\n",
    "        bearing BIGINT,\n",
    "        trip_id BIGINT,\n",
    "        base_version BIGINT,\n",
    "        destination_naptan_id STRING,\n",
    "        destination_name STRING,\n",
    "        event_timestamp TIMESTAMP,\n",
    "        time_to_station BIGINT,\n",
    "        current_location STRING,\n",
    "        towards STRING,\n",
    "        expected_arrival TIMESTAMP,\n",
    "        time_to_live TIMESTAMP\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.london_boroughs(\n",
    "        borough_code STRING,\n",
    "        borough_name STRING,\n",
    "        hectares DOUBLE,\n",
    "        shape_area DOUBLE,\n",
    "        shape_length DOUBLE,\n",
    "        geometry_geojson STRING\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.stop_points(\n",
    "        naptan_id STRING,\n",
    "        indicator STRING,\n",
    "        ics_code BIGINT,\n",
    "        stop_type STRING,\n",
    "        common_name STRING,\n",
    "        longitude DOUBLE,\n",
    "        latitude DOUBLE\n",
    "    )    \n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.bus_arrival_events(\n",
    "        arrival_event_id BIGINT, \n",
    "        line_id STRING,\n",
    "        vehicle_id STRING,\n",
    "        naptan_id STRING,\n",
    "        station_name STRING,\n",
    "        platform_name STRING,\n",
    "        direction STRING,\n",
    "        destination_name STRING,\n",
    "        time_to_station BIGINT,\n",
    "        expected_arrival TIMESTAMP,\n",
    "        time_to_live TIMESTAMP,\n",
    "        is_service_disrupted BOOLEAN,\n",
    "        severity_code BIGINT, \n",
    "        severity_description STRING, \n",
    "        event_timestamp TIMESTAMP\n",
    "    )     \n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.bus_stops_geo(\n",
    "        naptan_id STRING,\n",
    "        stop_name STRING,\n",
    "        stop_type STRING,\n",
    "        borough_code STRING,\n",
    "        borough_name STRING,\n",
    "        longitude DOUBLE,\n",
    "        latitude DOUBLE\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_silver}.line_disruption_geo(\n",
    "        line_id STRING,\n",
    "        service_type STRING,\n",
    "        severity_code BIGINT,\n",
    "        severity_description STRING,\n",
    "        disruption_category STRING,\n",
    "        disruption_description STRING,\n",
    "        disruption_from_date TIMESTAMP,\n",
    "        disruption_to_date TIMESTAMP,\n",
    "        is_service_disrupted BOOLEAN,\n",
    "        borough_code STRING,\n",
    "        borough_name STRING,\n",
    "        longitude DOUBLE,\n",
    "        latitude DOUBLE,\n",
    "        event_timestamp TIMESTAMP\n",
    "    )\n",
    "    \"\"\")\n",
    "    print(\"\\n✅ Ensured all silver tables exist\")\n",
    "\n",
    "\n",
    "# Helper function to perform upsert using a merge query in foreachBatch\n",
    "def upserter(df_micro_batch, batch_id, merge_query, temp_view):\n",
    "    df_micro_batch.createOrReplaceTempView(temp_view)\n",
    "    df_micro_batch._jdf.sparkSession().sql(merge_query)\n",
    "    print(f\"Batch {batch_id} for {temp_view} processed.\")\n",
    "\n",
    "\n",
    "# Stream upsert for line_status table\n",
    "def upsert_line_status(once = True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {schema_silver}.line_status AS target\n",
    "    USING line_status_delta AS source\n",
    "    ON target.line_id = source.line_id \n",
    "    AND target.event_timestamp = source.event_timestamp\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    df_silver = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{schema_bronze}.line_status_bz\")\n",
    "            .select(\n",
    "                col(\"id\").alias(\"line_id\"),\n",
    "                get(col(\"serviceTypes\"),0)[\"name\"].alias(\"service_type\"),\n",
    "                get(col(\"lineStatuses\"), 0)[\"statusSeverity\"].alias(\"severity_code\"),\n",
    "                get(col(\"lineStatuses\"), 0)[\"statusSeverityDescription\"].alias(\"severity_description\"),\n",
    "                get(col(\"lineStatuses\"), 0)[\"disruption\"][\"category\"].alias(\"disruption_category\"),\n",
    "                get(col(\"lineStatuses\"), 0)[\"disruption\"][\"description\"].alias(\"disruption_description\"),\n",
    "                to_timestamp(get(get(col(\"lineStatuses\"), 0)[\"validityPeriods\"], 0)[\"fromDate\"]).alias(\"disruption_from_date\"),\n",
    "                to_timestamp(get(get(col(\"lineStatuses\"), 0)[\"validityPeriods\"], 0)[\"toDate\"]).alias(\"disruption_to_date\"),\n",
    "                get(get(col(\"lineStatuses\"), 0)[\"validityPeriods\"], 0)[\"isNow\"].alias(\"is_service_disrupted\"),\n",
    "                col(\"created\").cast(\"timestamp\").alias(\"event_timestamp\")\n",
    "            )\n",
    "            .withWatermark(\"event_timestamp\", \"30 seconds\")\n",
    "            .dropDuplicates([\"line_id\", \"event_timestamp\"])\n",
    "    )\n",
    "\n",
    "    stream_writer = (df_silver.writeStream\n",
    "            .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"line_status_delta\"))\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{silver_checkpoint}/line_status\") \n",
    "            .queryName(\"line_status_upsert_stream\")   \n",
    "\n",
    "    )\n",
    "\n",
    "    if once:\n",
    "        stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "# Stream upsert for bus_arrivals table\n",
    "def upsert_bus_arrivals(once = True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {schema_silver}.bus_arrivals AS target \n",
    "    USING bus_arrivals_delta AS source\n",
    "    ON target.arrival_id = source.arrival_id\n",
    "    AND target.event_timestamp = source.event_timestamp\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    df_silver = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{schema_bronze}.bus_arrivals_bz\")\n",
    "            .select(\n",
    "                col(\"id\").alias(\"arrival_id\"),\n",
    "                col(\"operationType\").alias(\"operation_type\"),\n",
    "                col(\"vehicleId\").alias(\"vehicle_id\"),\n",
    "                col(\"naptanId\").alias(\"naptan_id\"),\n",
    "                col(\"stationName\").alias(\"station_name\"),\n",
    "                col(\"lineId\").alias(\"line_id\"),\n",
    "                col(\"platformName\").alias(\"platform_name\"),\"direction\",\"bearing\",\n",
    "                col(\"tripId\").alias(\"trip_id\"),\n",
    "                col(\"baseVersion\").alias(\"base_version\"),\n",
    "                col(\"destinationNaptanId\").alias(\"destination_naptan_id\"),\n",
    "                col(\"destinationName\").alias(\"destination_name\"),\n",
    "                col(\"timestamp\").cast(\"timestamp\").alias(\"event_timestamp\"),\n",
    "                col(\"timeToStation\").alias(\"time_to_station\"),\n",
    "                col(\"currentLocation\").alias(\"current_location\"),\"towards\",\n",
    "                col(\"expectedArrival\").cast(\"timestamp\").alias(\"expected_arrival\"),\n",
    "                col(\"timeToLive\").cast(\"timestamp\").alias(\"time_to_live\")\n",
    "            )\n",
    "            .withWatermark(\"event_timestamp\", \"30 seconds\")\n",
    "            .dropDuplicates([\"arrival_id\", \"event_timestamp\"])\n",
    "    )\n",
    "\n",
    "    stream_writer = (df_silver.writeStream\n",
    "                    .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"bus_arrivals_delta\"))\n",
    "                    .outputMode(\"update\")\n",
    "                    .option(\"checkpointLocation\", f\"{silver_checkpoint}/bus_arrivals\") \n",
    "                    .queryName(\"bus_arrival_upsert_stream\")\n",
    "    )\n",
    "\n",
    "    if once:\n",
    "        stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "# Stream upsert for stop_points table\n",
    "def upsert_stop_points(once = True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {schema_silver}.stop_points AS target\n",
    "    USING stop_points_delta AS source\n",
    "    ON target.naptan_id = source.naptan_id\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    df_silver = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDelete\", True)\n",
    "            .table(f\"{schema_bronze}.stop_points_bz\")\n",
    "            .select(explode_outer(col(\"stopPoints\")).alias(\"stop\"))\n",
    "            .select(\n",
    "                col(\"stop.naptanId\").alias(\"naptan_id\"),\n",
    "                split(col(\"stop.indicator\"), \",\")[0].alias(\"indicator\"),\n",
    "                col(\"stop.icsCode\").cast(\"bigint\").alias(\"ics_code\"),\n",
    "                split(col(\"stop.stopType\"), \",\")[0].alias(\"stop_type\"),\n",
    "                col(\"stop.hubNaptanCode\").alias(\"hub_naptan_code\"),\n",
    "                split(col(\"stop.commonName\"), \",\")[0].alias(\"common_name\"),\n",
    "                col(\"stop.lon\").alias(\"longitude\"),\n",
    "                col(\"stop.lat\").alias(\"latitude\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "    stream_writer = (df_silver.writeStream\n",
    "            .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"stop_points_delta\"))\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{silver_checkpoint}/stop_points\") \n",
    "            .queryName(\"stop_points_upsert_stream\")   \n",
    "\n",
    "    )\n",
    "\n",
    "    if once:\n",
    "        stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "# Batch write for london_boroughs table\n",
    "def write_london_boroughs():\n",
    "    df_silver = (\n",
    "        spark.read\n",
    "            .table(f\"{schema_bronze}.london_boroughs_bz\")\n",
    "            .select(\n",
    "                col(\"properties.CODE\").alias(\"borough_code\"),\n",
    "                col(\"properties.BOROUGH\").alias(\"borough_name\"),\n",
    "                col(\"properties.HECTARES\").alias(\"hectares\"),\n",
    "                col(\"properties.Shape__Area\").alias(\"shape_area\"),\n",
    "                col(\"properties.Shape__Length\").alias(\"shape_length\"),\n",
    "                to_json(\n",
    "                    expr(\"named_struct('type','Polygon','coordinates', geometry)\")\n",
    "                ).alias(\"geometry_geojson\")\n",
    "            )\n",
    "            .dropna(subset=[\"borough_code\"])\n",
    "            .dropDuplicates([\"borough_code\"])\n",
    "    )\n",
    "\n",
    "    \n",
    "    (\n",
    "    df_silver.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(f\"{schema_silver}.london_boroughs\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Batch upsert for bus_arrival_events derived table\n",
    "def upsert_bus_arrival_events_batch():\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {schema_silver}.bus_arrival_events AS target\n",
    "        USING (\n",
    "            SELECT\n",
    "                ba.arrival_id AS arrival_event_id,\n",
    "                ba.line_id,\n",
    "                ba.vehicle_id,\n",
    "                ba.naptan_id,\n",
    "                ba.station_name,\n",
    "                ba.platform_name,\n",
    "                ba.direction,\n",
    "                ba.destination_name,\n",
    "                ba.time_to_station,\n",
    "                ba.expected_arrival,\n",
    "                ba.time_to_live,\n",
    "                COALESCE(ls.is_service_disrupted, FALSE) AS is_service_disrupted,\n",
    "                ls.severity_code,\n",
    "                ls.severity_description,\n",
    "                ba.event_timestamp\n",
    "            FROM {schema_silver}.bus_arrivals ba\n",
    "            LEFT JOIN {schema_silver}.line_status ls\n",
    "              ON ba.line_id = ls.line_id\n",
    "             AND ba.event_timestamp BETWEEN\n",
    "                 ls.event_timestamp - INTERVAL 30 SECONDS\n",
    "             AND ls.event_timestamp + INTERVAL 30 SECONDS\n",
    "        ) AS source\n",
    "        ON target.arrival_event_id = source.arrival_event_id\n",
    "       AND target.event_timestamp   = source.event_timestamp\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Batch upsert for bus_stops_geo derived table (spatial enrichment)\n",
    "def upsert_bus_stops_geo_batch():\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {schema_silver}.bus_stops_geo AS target\n",
    "    USING bus_stops_geo_delta AS source\n",
    "    ON target.naptan_id = source.naptan_id\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "    # Read stop_points and create geometry\n",
    "    df_stop_point = (\n",
    "        spark.read\n",
    "            .table(f\"{schema_silver}.stop_points\")\n",
    "            .select(\n",
    "                \"naptan_id\",\n",
    "                col(\"common_name\").alias(\"stop_name\"),\"stop_type\",\"longitude\",\"latitude\",\n",
    "                expr(\"ST_SetSRID(ST_Point(longitude, latitude), 4326)\").alias(\"stop_geom\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Read london_boroughs and create geometry\n",
    "    df_london_borough = (\n",
    "        spark.read\n",
    "            .table(f\"{schema_silver}.london_boroughs\")\n",
    "            .select(\n",
    "                \"borough_code\",col(\"borough_name\"),\n",
    "                expr(\n",
    "                    \"ST_GeomFromGeoJSON(\"\n",
    "                    \"to_json(named_struct('type','Polygon','coordinates',\"\n",
    "                    \"from_json(geometry_geojson,\"\n",
    "                    \"'struct<coordinates:struct<coordinates:array<array<array<double>>>>>')\"\n",
    "                    \".coordinates.coordinates)))\"\n",
    "                ).alias(\"borough_geom\")\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Spatial join to assign borough to stop\n",
    "    df_silver = (\n",
    "        df_stop_point\n",
    "            .join(broadcast(df_london_borough),expr(\"ST_Contains(borough_geom, stop_geom)\"),\"inner\")\n",
    "            .select(\"naptan_id\",\"stop_name\",\"stop_type\",\"borough_code\",\"borough_name\",\"longitude\",\"latitude\")\n",
    "    )\n",
    "\n",
    "    df_silver.createOrReplaceTempView(\"bus_stops_geo_delta\")\n",
    "    spark.sql(merge_query)\n",
    "\n",
    "\n",
    "# Batch upsert for line_disruption_geo table (spatial enrichment)\n",
    "def upsert_line_disruption_geo_batch():\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {schema_silver}.line_disruption_geo AS target\n",
    "        USING (\n",
    "            SELECT\n",
    "                ls.line_id,\n",
    "                ls.service_type,\n",
    "                ls.severity_code,\n",
    "                ls.severity_description,\n",
    "                ls.disruption_category,\n",
    "                ls.disruption_description,\n",
    "                ls.disruption_from_date,\n",
    "                ls.disruption_to_date,\n",
    "                ls.is_service_disrupted,\n",
    "                bs.borough_code,\n",
    "                bs.borough_name,\n",
    "                bs.longitude,\n",
    "                bs.latitude,\n",
    "                ls.event_timestamp\n",
    "            FROM {schema_silver}.line_status ls\n",
    "            JOIN {schema_silver}.bus_stops_geo bs\n",
    "        ) AS source\n",
    "        ON  target.line_id = source.line_id\n",
    "        AND target.disruption_description = source.disruption_description\n",
    "        AND target.borough_name = source.borough_name\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# Await all active streaming queries if once=True\n",
    "def await_queries(once):\n",
    "    if once:\n",
    "        for q in spark.streams.active:\n",
    "            q.awaitTermination()\n",
    "\n",
    "\n",
    "# Orchestrate all silver streaming and batch upserts\n",
    "def upsert_silver(once=True, processing_time=\"5 seconds\"):\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "\n",
    "    ensure_silver_tables()\n",
    "\n",
    "    print(\"Running Silver streaming layer...\")\n",
    "    upsert_line_status(once, processing_time)\n",
    "    upsert_bus_arrivals(once, processing_time)\n",
    "    upsert_stop_points(once, processing_time)\n",
    "\n",
    "    await_queries(once)\n",
    "    print(f\"Completed silver streaming layer in {int(time.time()) - start} seconds\")\n",
    "\n",
    "    write_london_boroughs()\n",
    "\n",
    "    print(\"Processing derived table...\")\n",
    "    upsert_bus_arrival_events_batch()\n",
    "    upsert_bus_stops_geo_batch()\n",
    "    upsert_line_disruption_geo_batch()\n",
    "    print(f\"✅ Completed silver batch enrichment layer in {int(time.time()) - start} seconds\")\n",
    "\n",
    "    \n",
    "\n",
    "# Assert that a table has at least min_count records\n",
    "def assert_count(schema_silver, table_name, min_count=1):\n",
    "    print(f\"Validating record counts in {table_name}...\", end=\"\")\n",
    "    actual_count = spark.read.table(f\"{schema_silver}.{table_name}\").where(filter_expr).count()\n",
    "    assert actual_count >= min_count, (f\"{table_name} has {actual} records, expected >= {min_count}\")\n",
    "    print(\"Success\")\n",
    "\n",
    "# Validate all silver tables for minimum record counts\n",
    "def validate_silver():\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    schema=schema_silver\n",
    "    print(\"\\nStarting Silver Layer Validation...\")\n",
    "\n",
    "    assert_count(schema_silver, \"line_status\")\n",
    "    assert_count(schema_silver, \"bus_arrivals\")\n",
    "    assert_count(schema_silver, \"london_boroughs\")\n",
    "    assert_count(schema_silver, \"stop_points\")\n",
    "\n",
    "    assert_count(schema_silver, \"bus_arrival_events\")\n",
    "    assert_count(schema_silver, \"bus_stops_geo\")\n",
    "\n",
    "    assert_count(schema_silver, \"line_disruption_geo\")\n",
    "\n",
    "    print(f\"✅ Silver Layer Validation completed in {int(time.time()) - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
