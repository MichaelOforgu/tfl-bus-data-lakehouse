{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6161cc6b-be35-436b-ba47-d38d9226637a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c004f3-be0e-46bf-a242-1a4c66999a7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup environment and bootstrap"
    }
   },
   "outputs": [],
   "source": [
    "# Set up environment widget and get value\n",
    "try:\n",
    "    dbutils.widgets.text(\"Environment\", \"dev\", \"Set the current environment\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ENV = dbutils.widgets.get(\"Environment\").strip().lower()\n",
    "\n",
    "if ENV not in (\"dev\", \"prod\"):\n",
    "    raise ValueError(\"env must be 'dev' or 'prod'\")\n",
    "\n",
    "# Set storage account per environment\n",
    "STORAGE_ACCOUNT = {\n",
    "    \"dev\":  \"sttfldevuks\",\n",
    "    \"prod\": \"sttflproduks\",\n",
    "}[ENV]\n",
    "\n",
    "# Set base zone URLs\n",
    "data_zone_url = f\"abfss://tfl-{ENV}@{STORAGE_ACCOUNT}.dfs.core.windows.net/data-zone\"\n",
    "checkpoint_zone_url = f\"abfss://tfl-{ENV}@{STORAGE_ACCOUNT}.dfs.core.windows.net/checkpoint-zone\"\n",
    "\n",
    "# Set derived paths\n",
    "raw_url = f\"{data_zone_url}/raw\"\n",
    "test_data_url = f\"{data_zone_url}/test_data\"\n",
    "checkpoint_url = f\"{checkpoint_zone_url}/checkpoints\"\n",
    "\n",
    "# Bootstrap infrastructure\n",
    "\n",
    "# Create catalog\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "# Create schemas\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_landing}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_bronze}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_silver}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_gold}\")\n",
    "\n",
    "# Create external volumes\n",
    "spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {raw_volume}  LOCATION '{raw_url}'\")\n",
    "spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {checkpoint_volume} LOCATION '{checkpoint_url}'\")\n",
    "if ENV != \"prod\":\n",
    "    spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {test_volume} LOCATION '{test_data_url}'\")\n",
    "\n",
    "# Check accessibility of expected raw folders\n",
    "for name, path in source_subdir.items():\n",
    "    try:\n",
    "        dbutils.fs.ls(path + \"/\")\n",
    "        print(f\"✅ ok: {name} -> {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ not accessible yet: {name} -> {path} | {e}\")\n",
    "\n",
    "print(\"Bootstrap complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e522ad-8a2e-4061-96d1-e19cec6e9215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP CATALOG IF EXISTS tfl_pipeline CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a1959b-9343-428f-8d73-2ad2483afd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Setup flag to track initialization state\n",
    "# initialized = False\n",
    "\n",
    "# def create_catalog(catalog):\n",
    "#     # Create catalog if it does not exist\n",
    "#     print(f\"Creating catalog {catalog}...\", end=\"\")\n",
    "#     spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "#     print(\"Done\")\n",
    "\n",
    "\n",
    "# def create_schemas(catalog, schemas):\n",
    "#     # Create each schema in the provided list if it does not exist\n",
    "#     for schema in schemas:\n",
    "#         print(f\"Creating schema {catalog}.{schema}...\", end=\"\")\n",
    "#         spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "#         print(\"Done\")\n",
    "\n",
    "\n",
    "# def create_landing_volumes(catalog, schema, volumes):\n",
    "#     # Create external volumes for landing data\n",
    "#     for volume_name, volume_path in volumes.items():\n",
    "#         print(f\"Creating volume {catalog}.{schema}.{volume_name}...\", end=\"\")\n",
    "#         spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {catalog}.{schema}.{volume_name} LOCATION '{volume_path}'\")\n",
    "#         print(\"Done\")\n",
    "\n",
    "\n",
    "\n",
    "# def create_line_status(catalog, schema=silver_schema):\n",
    "#     # Create the line_status table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.line_status...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.line_status(\n",
    "#                 line_id STRING,\n",
    "#                 service_type STRING,\n",
    "#                 severity_code BIGINT,\n",
    "#                 severity_description STRING,\n",
    "#                 disruption_category STRING,\n",
    "#                 disruption_description STRING,\n",
    "#                 disruption_from_date TIMESTAMP,\n",
    "#                 disruption_to_date TIMESTAMP,\n",
    "#                 is_service_disrupted BOOLEAN,\n",
    "#                 event_timestamp TIMESTAMP\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "# def create_bus_arrivals(catalog, schema=silver_schema):\n",
    "#     # Create the bus_arrivals table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.bus_arrivals...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.bus_arrivals(\n",
    "#                 arrival_id STRING,\n",
    "#                 operation_type BIGINT,\n",
    "#                 vehicle_id STRING,\n",
    "#                 naptan_id STRING,\n",
    "#                 station_name STRING,\n",
    "#                 line_id STRING,\n",
    "#                 platform_name STRING,\n",
    "#                 direction STRING,\n",
    "#                 bearing BIGINT,\n",
    "#                 trip_id BIGINT,\n",
    "#                 base_version BIGINT,\n",
    "#                 destination_naptan_id STRING,\n",
    "#                 destination_name STRING,\n",
    "#                 event_timestamp TIMESTAMP,\n",
    "#                 time_to_station BIGINT,\n",
    "#                 current_location STRING,\n",
    "#                 towards STRING,\n",
    "#                 expected_arrival TIMESTAMP,\n",
    "#                 time_to_live TIMESTAMP\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "\n",
    "# def create_london_boroughs(catalog, schema=silver_schema):\n",
    "#     # Create the london_boroughs table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.london_boroughs...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.london_boroughs(\n",
    "#                 borough_code STRING,\n",
    "#                 borough_name STRING,\n",
    "#                 hectares DOUBLE,\n",
    "#                 shape_area DOUBLE,\n",
    "#                 shape_length DOUBLE,\n",
    "#                 geometry_geojson STRING\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "\n",
    "# def create_stop_points(catalog, schema=silver_schema):\n",
    "#     # Create the stop_points table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.stop_points...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.stop_points(\n",
    "#                 naptan_id STRING,\n",
    "#                 indicator STRING,\n",
    "#                 ics_code BIGINT,\n",
    "#                 stop_type STRING,\n",
    "#                 common_name STRING,\n",
    "#                 longitude DOUBLE,\n",
    "#                 latitude DOUBLE\n",
    "#             )    \n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "\n",
    "# def create_bus_arrival_events(catalog, schema=silver_schema):\n",
    "#     # Create the bus_arrival_events table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.bus_arrival_events...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.bus_arrival_events(\n",
    "#                 arrival_event_id BIGINT, \n",
    "#                 line_id STRING,\n",
    "#                 vehicle_id STRING,\n",
    "#                 naptan_id STRING,\n",
    "#                 station_name STRING,\n",
    "#                 platform_name STRING,\n",
    "#                 direction STRING,\n",
    "#                 destination_name STRING,\n",
    "#                 time_to_station BIGINT,\n",
    "#                 expected_arrival TIMESTAMP,\n",
    "#                 time_to_live TIMESTAMP,\n",
    "#                 is_service_disrupted BOOLEAN,\n",
    "#                 severity_code BIGINT, \n",
    "#                 severity_description STRING, \n",
    "#                 event_timestamp TIMESTAMP\n",
    "#             )     \n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "# def create_bus_stops_geo(catalog, schema=silver_schema):\n",
    "#     # Create the bus_stops_geo table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.bus_stops_geo...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.bus_stops_geo(\n",
    "#                 naptan_id STRING,\n",
    "#                 stop_name STRING,\n",
    "#                 stop_type STRING,\n",
    "#                 borough_code STRING,\n",
    "#                 borough_name STRING,\n",
    "#                 longitude DOUBLE,\n",
    "#                 latitude DOUBLE\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "# def create_line_disruption_geo(catalog, schema=silver_schema):\n",
    "#     # Create the line_disruption_geo table if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.line_disruption_geo...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.line_disruption_geo(\n",
    "#                 line_id STRING,\n",
    "#                 service_type STRING,\n",
    "#                 severity_code BIGINT,\n",
    "#                 severity_description STRING,\n",
    "#                 disruption_category STRING,\n",
    "#                 disruption_description STRING,\n",
    "#                 disruption_from_date TIMESTAMP,\n",
    "#                 disruption_to_date TIMESTAMP,\n",
    "#                 is_service_disrupted BOOLEAN,\n",
    "#                 borough_code STRING,\n",
    "#                 borough_name STRING,\n",
    "#                 longitude DOUBLE,\n",
    "#                 latitude DOUBLE,\n",
    "#                 event_timestamp TIMESTAMP\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\"❌ Application database is not defined. Cannot create table in undefined database.\")\n",
    "\n",
    "\n",
    "# def create_bus_disruption_impact(catalog, schema=gold_schema):\n",
    "#     # Create the disruption_impact table in the gold schema if initialized\n",
    "#     if initialized:\n",
    "#         print(f\"Creating table {catalog}.{schema}.disruption_impact...\", end=\"\")\n",
    "#         spark.sql(f\"\"\"\n",
    "#             CREATE OR REPLACE TABLE {catalog}.{schema}.disruption_impact (\n",
    "#                 line_id STRING,\n",
    "#                 borough_name STRING,\n",
    "#                 disruption_count BIGINT,\n",
    "#                 max_severity_code BIGINT,\n",
    "#                 max_severity_description STRING,\n",
    "#                 disruption_start TIMESTAMP,\n",
    "#                 disruption_end TIMESTAMP,\n",
    "#                 service_disruption_events BIGINT\n",
    "#             )\n",
    "#         \"\"\")\n",
    "#         print(\"Done\")\n",
    "#     else:\n",
    "#         raise ReferenceError(\n",
    "#             \"❌ Application database is not defined. Cannot create table in undefined database.\"\n",
    "#         )\n",
    "\n",
    "# def setup_infrastructure(catalog):\n",
    "#     # Set up the platform infrastructure: catalog, schemas, volumes, and tables\n",
    "#     import time\n",
    "#     global initialized\n",
    "\n",
    "#     start = int(time.time())\n",
    "#     print(\"Starting platform setup...\")\n",
    "\n",
    "#     create_catalog(catalog)\n",
    "#     create_schemas(catalog, schemas)\n",
    "#     create_landing_volumes(catalog=catalog,schema=landing_schema,volumes=landing_volumes)\n",
    "\n",
    "#     initialized = True  # Set flag to allow table creation\n",
    "\n",
    "#     # ---- Silver layer ----\n",
    "#     create_line_status(catalog)\n",
    "#     create_bus_arrivals(catalog)\n",
    "#     create_london_boroughs(catalog)\n",
    "#     create_stop_points(catalog)\n",
    "#     create_bus_arrival_events(catalog)\n",
    "#     create_bus_stops_geo(catalog)\n",
    "#     create_line_disruption_geo(catalog)\n",
    "\n",
    "#     print(f\"Platform setup completed in {int(time.time()) - start} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# def assert_table(catalog, schema, table_name):\n",
    "#     # Assert that a table exists in the given catalog and schema\n",
    "#     exists = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\") \\\n",
    "#                 .filter(f\"isTemporary = false AND tableName = '{table_name}'\") \\\n",
    "#                 .count() == 1\n",
    "#     assert exists, f\"Table {catalog}.{schema}.{table_name} does not exist\"\n",
    "#     print(f\"Found table {catalog}.{schema}.{table_name}: Success\")\n",
    "\n",
    "\n",
    "\n",
    "# def validate_setup(catalog):\n",
    "#     # Validate that all required schemas and tables exist\n",
    "#     import time\n",
    "#     start = int(time.time())\n",
    "#     print(\"\\nStarting setup validation ...\")\n",
    "\n",
    "#     # ---- Silver ----\n",
    "#     schema = silver_schema\n",
    "#     schema_exists = spark.sql(f\"SHOW SCHEMAS IN {catalog}\") \\\n",
    "#         .filter(f\"databaseName = '{schema}'\") \\\n",
    "#         .count() == 1\n",
    "\n",
    "#     assert schema_exists, f\"The schema '{catalog}.{schema}' is missing\"\n",
    "#     print(f\"Found schema {catalog}.{schema}: Success\")\n",
    "\n",
    "#     assert_table(catalog, schema, \"line_status\")\n",
    "#     assert_table(catalog, schema, \"bus_arrivals\")\n",
    "#     assert_table(catalog, schema, \"london_boroughs\")\n",
    "#     assert_table(catalog, schema, \"stop_points\")\n",
    "#     assert_table(catalog, schema, \"bus_arrival_events\")\n",
    "#     assert_table(catalog, schema, \"bus_stops_geo\")\n",
    "#     assert_table(catalog, schema, \"line_disruption_geo\")\n",
    "\n",
    "#     print(f\"Setup validation completed in {int(time.time()) - start} seconds\")\n",
    "\n",
    "\n",
    "# def cleanup(catalog, schema):\n",
    "#     # Drop the specified schema and all its contents if it exists\n",
    "#     try:\n",
    "#         schema_exists = spark.sql(f\"SHOW SCHEMAS IN {catalog}\") \\\n",
    "#                     .filter(f\"schemaName = '{schema}'\") \\\n",
    "#                     .count()==1\n",
    "\n",
    "#         if schema_exists:\n",
    "#             print(f\"Dropping schema {catalog}.{schema}...\", end=\"\")\n",
    "#             spark.catalog.clearCache()\n",
    "#             spark.sql(f\"DROP SCHEMA {catalog}.{schema} CASCADE\")\n",
    "#             print(\"Done\")\n",
    "#         else:\n",
    "#             print(f\"Schema {catalog}.{schema} does not exist. Skipping drop.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error while dropping schema {catalog}.{schema}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-setup",
   "widgets": {
    "Environment": {
     "currentValue": "dev",
     "nuid": "592f59b4-cd3c-40c1-9d26-26ed3b266395",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Set the current environment/catalog name",
      "name": "Environment",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dev",
      "label": "Set the current environment/catalog name",
      "name": "Environment",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
