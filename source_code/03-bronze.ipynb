{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f95049e-875f-47ea-aac2-5f96195e78e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09ade55-e240-4dd3-8d83-e16e6fcdb374",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def consume_autoloader(*, source_subdir, target_table, once = True, processing_time = \"5 seconds\"):\n",
    "\n",
    "    # Define base volume and paths for raw data, checkpoints, and schema evolution\n",
    "    file_path = f\"{raw_base_path}/{source_subdir}\"\n",
    "    bronze_checkpoint = (f\"{checkpoint_base_path}/bronze/{target_table}\")\n",
    "    schema_path = (f\"{checkpoint_base_path}/schemas/{target_table}\")\n",
    "\n",
    "    # Read raw stream data into a dataframe using Autoloader\n",
    "    df_stream = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "            .load(file_path)\n",
    "            .withColumn(\"_ingest_time\", F.current_timestamp())  # Add ingestion timestamp\n",
    "            .withColumn(\"_source_file\", F.input_file_name())    # Add source file name\n",
    "    )\n",
    "\n",
    "    # Write streaming data to delta table\n",
    "    stream_writer = (\n",
    "        df_stream.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", bronze_checkpoint)\n",
    "            .queryName(target_table)\n",
    "    )\n",
    "\n",
    "    # Trigger streaming write either once or continuously\n",
    "    if once:\n",
    "        stream_writer.trigger(availableNow=True).toTable(f\"{schema_bronze}.{target_table}\")\n",
    "    else:\n",
    "        stream_writer.trigger(processingTime=processing_time).toTable(f\"{schema_bronze}.{target_table}\")\n",
    "\n",
    "\n",
    "\n",
    "def consume_bus_arrivals(once=True, processing_time=\"5 seconds\"):\n",
    "    # Consume bus arrivals data into bronze table\n",
    "    consume_autoloader(\n",
    "        source_subdir=\"arrivals\",\n",
    "        target_table=\"bus_arrivals_bz\",\n",
    "        once=once,\n",
    "        processing_time=processing_time,\n",
    "    )\n",
    "\n",
    "\n",
    "def consume_line_status(once=True, processing_time=\"5 seconds\"):\n",
    "    # Consume line status data into bronze table\n",
    "    consume_autoloader(\n",
    "        source_subdir=\"line_status\",\n",
    "        target_table=\"line_status_bz\",\n",
    "        once=once,\n",
    "        processing_time=processing_time,\n",
    "    )\n",
    "\n",
    "def consume_stop_points(once=True, processing_time=\"5 seconds\"):\n",
    "    # Consume stop points data into bronze table\n",
    "    consume_autoloader(\n",
    "        source_subdir=\"stop_points\",\n",
    "        target_table=\"stop_points_bz\",\n",
    "        once=once,\n",
    "        processing_time=processing_time,\n",
    "    )\n",
    "\n",
    "\n",
    "def consume_london_boroughs(once=True, processing_time=\"5 seconds\"):\n",
    "    # Consume London boroughs data into bronze table\n",
    "    consume_autoloader(\n",
    "        source_subdir=\"london_boroughs\",\n",
    "        target_table=\"london_boroughs_bz\",\n",
    "        once=once,\n",
    "        processing_time=processing_time,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Orchestrate bronze layer consumption\n",
    "def consume_bronze(once=True, processing_time=\"5 seconds\"):\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(\"\\nStarting bronze layer consumption...\")\n",
    "\n",
    "    # Consume all bronze tables\n",
    "    consume_bus_arrivals(once, processing_time)\n",
    "    consume_line_status(once, processing_time)\n",
    "    consume_stop_points(once, processing_time)\n",
    "    consume_london_boroughs(once, processing_time)\n",
    "\n",
    "    # Wait for all streams to finish if running in 'once' mode\n",
    "    if once:\n",
    "        for stream in spark.streams.active:\n",
    "            stream.awaitTermination()\n",
    "\n",
    "    print(f\"✅ Completed bronze layer consumption in {int(time.time()) - start} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# Bronze layer validation\n",
    "def assert_bronze_table(schema_bronze, table_name):\n",
    "    # Validate that the bronze table is not empty and has no missing _source_file\n",
    "    print(f\"Validating {schema_bronze}.{table_name}...\")\n",
    "    df = spark.table(f\"{schema_bronze}.{table_name}\")\n",
    "    stats = df.selectExpr(\n",
    "            \"count(*) as total_rows\",\n",
    "            \"sum(CASE WHEN _source_file IS NULL THEN 1 ELSE 0 END) as missing_source\"\n",
    "        ).collect()[0]\n",
    "    assert stats.total_rows > 0, f\"{table_name} is empty\"\n",
    "    assert stats.missing_source == 0, f\"{table_name} has missing _source_file\"\n",
    "    print(f\"{table_name}: {stats.total_rows:,} records validated\")\n",
    "\n",
    "\n",
    "def validate_bronze():\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(\"\\nValidating bronze layer records...\")\n",
    "\n",
    "    # Validate all bronze tables\n",
    "    assert_bronze_table(schema_bronze, \"bus_arrivals_bz\")\n",
    "    assert_bronze_table(schema_bronze, \"line_status_bz\")\n",
    "    assert_bronze_table(schema_bronze, \"stop_points_bz\")\n",
    "    assert_bronze_table(schema_bronze, \"london_boroughs_bz\")\n",
    "\n",
    "    print(f\"✅ Bronze layer validation completed in {int(time.time()) - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
