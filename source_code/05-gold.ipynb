{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc1e1d1-82e6-4bac-8927-4363d9d2bed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a750fd-f6dd-4f2d-ad32-d9bf0f3ea47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a VIEW for line reliability KPI in the Gold layer\n",
    "def create_line_reliability_kpi():\n",
    "    print(f\"Creating VIEW {schema_gold}.line_reliability_kpi...\", end=\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {schema_gold}.line_reliability_kpi AS\n",
    "        SELECT\n",
    "            line_id,\n",
    "            DATE(event_timestamp) AS service_date,\n",
    "            COUNT(*) AS total_arrivals,\n",
    "            SUM(CASE WHEN is_service_disrupted THEN 1 ELSE 0 END) AS disrupted_arrivals,\n",
    "            ROUND(1 - (SUM(CASE WHEN is_service_disrupted THEN 1 ELSE 0 END) / COUNT(*)),4) AS reliability_ratio,\n",
    "            CASE\n",
    "                WHEN (1 - (SUM(CASE WHEN is_service_disrupted THEN 1 ELSE 0 END) / COUNT(*))) >= 0.95\n",
    "                THEN 'meets_SLA'\n",
    "                ELSE 'breach'\n",
    "            END AS sla_status\n",
    "        FROM {schema_silver}.bus_arrival_events\n",
    "        GROUP BY line_id, DATE(event_timestamp)\n",
    "    \"\"\")\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "# Create a VIEW for line regularity KPI in the Gold layer\n",
    "def create_line_regularity_kpi():\n",
    "    print(f\"Creating VIEW {schema_gold}.line_regularity_kpi...\", end=\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {schema_gold}.line_regularity_kpi AS\n",
    "        SELECT\n",
    "            line_id,\n",
    "            DATE(event_timestamp) AS service_date,\n",
    "            AVG(time_to_station) AS avg_wait_seconds,\n",
    "            STDDEV(time_to_station) AS wait_time_variability,\n",
    "            CASE\n",
    "                WHEN STDDEV(time_to_station) <= 120 THEN 'good'\n",
    "                WHEN STDDEV(time_to_station) <= 300 THEN 'acceptable'\n",
    "                ELSE 'poor'\n",
    "            END AS regularity_status\n",
    "        FROM {schema_silver}.bus_arrival_events\n",
    "        GROUP BY line_id, DATE(event_timestamp)\n",
    "    \"\"\")\n",
    "    print(\"Done\")\n",
    "    \n",
    "    \n",
    "# Create a VIEW for line disruption impact KPI in the Gold layer\n",
    "def create_line_disruption_impact_kpi():\n",
    "    print(f\"Creating VIEW {schema_gold}.line_disruption_impact_kpi...\", end=\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {schema_gold}.line_disruption_impact_kpi AS\n",
    "        SELECT\n",
    "            line_id,\n",
    "            borough_name,\n",
    "            COUNT(DISTINCT disruption_description) AS disruption_count,\n",
    "            COUNT(*) AS service_disruption_events,\n",
    "            MAX(severity_code) AS max_severity_code,\n",
    "            ROUND(\n",
    "                COUNT(DISTINCT disruption_description)\n",
    "                * COUNT(*)\n",
    "                * MAX(severity_code),\n",
    "                0\n",
    "            ) AS disruption_impact_score,\n",
    "            CASE\n",
    "                WHEN MAX(severity_code) >= 10 THEN 'CRITICAL'\n",
    "                WHEN MAX(severity_code) >= 5 THEN 'HIGH'\n",
    "                ELSE 'LOW'\n",
    "            END AS impact_level\n",
    "        FROM {schema_silver}.line_disruption_geo\n",
    "        WHERE is_service_disrupted = true\n",
    "        GROUP BY line_id, borough_name;\n",
    "    \"\"\")\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "# Create a VIEW for borough service equity KPI in the Gold layer\n",
    "def create_borough_service_equity_kpi():\n",
    "    print(f\"Creating VIEW {schema_gold}.borough_service_equity_kpi...\", end=\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW {schema_gold}.borough_service_equity_kpi AS\n",
    "        SELECT\n",
    "            borough_name,\n",
    "            COUNT(DISTINCT line_id) AS affected_lines,\n",
    "            SUM(service_disruption_events) AS total_disruptions,\n",
    "            AVG(max_severity_code) AS avg_severity,\n",
    "            ROUND(SUM(service_disruption_events) / COUNT(DISTINCT line_id),2) AS disruption_density\n",
    "        FROM {schema_gold}.line_disruption_impact_kpi\n",
    "        GROUP BY borough_name;\n",
    "    \"\"\")\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "# Orchestrate creation of all Gold layer KPI VIEWs\n",
    "def create_gold_kpis():\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(\"\\nExecuting gold layer KPI creation...\")\n",
    "\n",
    "    create_line_reliability_kpi()\n",
    "    create_line_regularity_kpi()\n",
    "    create_line_disruption_impact_kpi()\n",
    "    create_borough_service_equity_kpi()\n",
    "\n",
    "    print(f\"✅ Completed gold layer in {int(time.time()) - start} seconds\")\n",
    "    \n",
    "\n",
    "# Validate existence and row count of all Gold layer KPI VIEWs\n",
    "def validate_gold_kpis():\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(\"\\nValidating Gold KPIs...\")\n",
    "\n",
    "    tables = [\"line_reliability_kpi\",\"line_regularity_kpi\",\"line_disruption_impact_kpi\",\"borough_service_equity_kpi\"]\n",
    "\n",
    "    for t in tables:\n",
    "        assert spark.sql(f\"SHOW TABLES IN {schema_gold}\") \\\n",
    "            .filter(f\"tableName='{t}' AND isTemporary=false\") \\\n",
    "            .count() == 1, f\"Missing table: {schema_gold}.{t}\"\n",
    "\n",
    "        rows = spark.table(f\"{schema_gold}.{t}\").count()\n",
    "        assert rows > 0, f\"Empty table: {schema_gold}.{t}\"\n",
    "\n",
    "        print(f\"{t}: OK ({rows:,} rows)\")\n",
    "\n",
    "    print(f\"✅ Gold KPI validation completed in {int(time.time()) - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
